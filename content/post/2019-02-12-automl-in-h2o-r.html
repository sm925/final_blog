---
title: "AutoML in H2o R"
author: "Suchait Mattoo"
date: '2019-02-12'
keywords: tech
slug: automl-in-h2o-r
tags:
- r
- automl
- h2o
- GGally
- ggcorr
categories:
- Machine Learning
- Diabetes Prediction
- Healthcare
---



<!--more-->
<pre class="r"><code>## loading packages required
library(data.table)
library(h2o)</code></pre>
<pre><code>## 
## ----------------------------------------------------------------------
## 
## Your next step is to start H2O:
##     &gt; h2o.init()
## 
## For H2O package documentation, ask for help:
##     &gt; ??h2o
## 
## After starting H2O, you can use the Web UI at http://localhost:54321
## For more information visit http://docs.h2o.ai
## 
## ----------------------------------------------------------------------</code></pre>
<pre><code>## 
## Attaching package: &#39;h2o&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:data.table&#39;:
## 
##     hour, month, week, year</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cor, sd, var</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     &amp;&amp;, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,
##     colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,
##     log10, log1p, log2, round, signif, trunc</code></pre>
<pre class="r"><code>library(GGally)</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre class="r"><code># https://www.kaggle.com/uciml/pima-indians-diabetes-database link to data
diab &lt;- read.csv(&quot;diabetes.csv&quot;, header = T, stringsAsFactors = F) # reading csv
summary(diab) # summary of data</code></pre>
<pre><code>##   Pregnancies        Glucose      BloodPressure    SkinThickness  
##  Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  
##  1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  
##  Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  
##  Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  
##  3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  
##  Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  
##     Insulin           BMI        DiabetesPedigreeFunction      Age       
##  Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  
##  1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  
##  Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  
##  Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  
##  3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  
##  Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  
##     Outcome     
##  Min.   :0.000  
##  1st Qu.:0.000  
##  Median :0.000  
##  Mean   :0.349  
##  3rd Qu.:1.000  
##  Max.   :1.000</code></pre>
<pre class="r"><code>str(diab) # structure of data</code></pre>
<pre><code>## &#39;data.frame&#39;:    768 obs. of  9 variables:
##  $ Pregnancies             : int  6 1 8 1 0 5 3 10 2 8 ...
##  $ Glucose                 : int  148 85 183 89 137 116 78 115 197 125 ...
##  $ BloodPressure           : int  72 66 64 66 40 74 50 0 70 96 ...
##  $ SkinThickness           : int  35 29 0 23 35 0 32 0 45 0 ...
##  $ Insulin                 : int  0 0 0 94 168 0 88 0 543 0 ...
##  $ BMI                     : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ...
##  $ DiabetesPedigreeFunction: num  0.627 0.351 0.672 0.167 2.288 ...
##  $ Age                     : int  50 31 32 21 33 30 26 29 53 54 ...
##  $ Outcome                 : int  1 0 1 0 1 0 1 0 1 1 ...</code></pre>
<pre class="r"><code>setDT(diab) # setting to data.table
setnames(diab, &quot;Outcome&quot;, &quot;diabetes&quot;) # changing column name
head(diab) # peek at data</code></pre>
<pre><code>##    Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI
## 1:           6     148            72            35       0 33.6
## 2:           1      85            66            29       0 26.6
## 3:           8     183            64             0       0 23.3
## 4:           1      89            66            23      94 28.1
## 5:           0     137            40            35     168 43.1
## 6:           5     116            74             0       0 25.6
##    DiabetesPedigreeFunction Age diabetes
## 1:                    0.627  50        1
## 2:                    0.351  31        0
## 3:                    0.672  32        1
## 4:                    0.167  21        0
## 5:                    2.288  33        1
## 6:                    0.201  30        0</code></pre>
<pre class="r"><code>ggcorr(diab) # coreelation among al the features</code></pre>
<p><img src="/post/2019-02-12-automl-in-h2o-r_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>diab &lt;- diab[, diabetes := as.factor(diabetes)] # target variable to factor
## train test split
nrows &lt;- nrow(diab)
index &lt;- sample(1:nrows, 0.8 * nrows)  
train &lt;- diab[index,]                 
test &lt;- diab[-index,]                  
y &lt;- &quot;diabetes&quot; # target column name
x &lt;- setdiff(names(train), c(&quot;diabetes&quot;)) # predictors
h2o.init() # initializing h2o</code></pre>
<pre><code>## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /var/folders/g5/nnm1ywj938q4ssqdkwvpx29w0000gn/T//RtmpG8GVOA/h2o_suchaitmattoo_started_from_r.out
##     /var/folders/g5/nnm1ywj938q4ssqdkwvpx29w0000gn/T//RtmpG8GVOA/h2o_suchaitmattoo_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: . Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         2 seconds 78 milliseconds 
##     H2O cluster timezone:       America/New_York 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.22.1.1 
##     H2O cluster version age:    1 month and 15 days  
##     H2O cluster name:           H2O_started_from_R_suchaitmattoo_ydr819 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   1.78 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20)</code></pre>
<pre class="r"><code>train &lt;- as.h2o(train) # converting to h2o object</code></pre>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre class="r"><code>test &lt;- as.h2o(test)</code></pre>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre class="r"><code>## training automl model i n h2o
aml &lt;- h2o.automl(y = y, x = x,
                  training_frame = train,
                  max_runtime_secs = 999999999,
                  max_models = 10,
                  seed = 1
)</code></pre>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |                                                                 |   1%
  |                                                                       
  |==                                                               |   2%
  |                                                                       
  |==                                                               |   3%
  |                                                                       
  |===                                                              |   4%
  |                                                                       
  |===                                                              |   5%
  |                                                                       
  |=======                                                          |  11%
  |                                                                       
  |========                                                         |  12%
  |                                                                       
  |=========                                                        |  13%
  |                                                                       
  |=========                                                        |  14%
  |                                                                       
  |==========                                                       |  16%
  |                                                                       
  |============                                                     |  18%
  |                                                                       
  |==============                                                   |  21%
  |                                                                       
  |===============                                                  |  24%
  |                                                                       
  |=================                                                |  26%
  |                                                                       
  |============================================================     |  92%
  |                                                                       
  |==============================================================   |  96%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre class="r"><code>rm(list=setdiff(ls(), c(&quot;train&quot;, &quot;test&quot;, &quot;aml&quot;))) # cleaning global environment
lb &lt;- aml@leaderboard
print(lb, n = nrow(lb)) # print lb</code></pre>
<pre><code>##                                               model_id       auc   logloss
## 1                     XGBoost_1_AutoML_20190212_143524 0.8433272 0.4681621
## 2                     XGBoost_2_AutoML_20190212_143524 0.8378409 0.4766075
## 3  StackedEnsemble_BestOfFamily_AutoML_20190212_143524 0.8350316 0.4821233
## 4     StackedEnsemble_AllModels_AutoML_20190212_143524 0.8338630 0.4836954
## 5                     XGBoost_3_AutoML_20190212_143524 0.8336384 0.4859875
## 6                         GBM_2_AutoML_20190212_143524 0.8236561 0.4948509
## 7                         GBM_4_AutoML_20190212_143524 0.8225968 0.4954717
## 8                         GBM_3_AutoML_20190212_143524 0.8201904 0.4985759
## 9            GLM_grid_1_AutoML_20190212_143524_model_1 0.8192290 0.5046075
## 10                        GBM_1_AutoML_20190212_143524 0.8188030 0.5069162
## 11                        XRT_1_AutoML_20190212_143524 0.8073642 0.8657843
## 12                        DRF_1_AutoML_20190212_143524 0.7965528 0.8787156
##    mean_per_class_error      rmse       mse
## 1             0.2235214 0.3904046 0.1524158
## 2             0.2267797 0.3950608 0.1560730
## 3             0.2290422 0.3959580 0.1567827
## 4             0.2296063 0.3970299 0.1576328
## 5             0.2243734 0.3992969 0.1594380
## 6             0.2383625 0.4019861 0.1615928
## 7             0.2382359 0.4018459 0.1614801
## 8             0.2373723 0.4034094 0.1627391
## 9             0.2530655 0.4064454 0.1651979
## 10            0.2402162 0.4088127 0.1671278
## 11            0.2471360 0.4104426 0.1684632
## 12            0.2752640 0.4179662 0.1746957
## 
## [12 rows x 6 columns]</code></pre>
<pre class="r"><code>aml@leader # leader</code></pre>
<pre><code>## Model Details:
## ==============
## 
## H2OBinomialModel: xgboost
## Model ID:  XGBoost_1_AutoML_20190212_143524 
## Model Summary: 
##   number_of_trees
## 1              75
## 
## 
## H2OBinomialMetrics: xgboost
## ** Reported on training data. **
## 
## MSE:  0.09311028
## RMSE:  0.3051398
## LogLoss:  0.3189803
## Mean Per-Class Error:  0.1313541
## AUC:  0.9534616
## pr_auc:  0.9223602
## Gini:  0.9069232
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##          0   1    Error     Rate
## 0      368  25 0.063613  =25/393
## 1       44 177 0.199095  =44/221
## Totals 412 202 0.112378  =69/614
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.516104 0.836879 152
## 2                       max f2  0.283093 0.893761 229
## 3                 max f0point5  0.597153 0.888768 130
## 4                 max accuracy  0.583551 0.889251 132
## 5                max precision  0.934599 1.000000   0
## 6                   max recall  0.085522 1.000000 341
## 7              max specificity  0.934599 1.000000   0
## 8             max absolute_mcc  0.583551 0.758790 132
## 9   max min_per_class_accuracy  0.398600 0.872774 188
## 10 max mean_per_class_accuracy  0.370969 0.876153 199
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## 
## H2OBinomialMetrics: xgboost
## ** Reported on cross-validation data. **
## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
## 
## MSE:  0.1524158
## RMSE:  0.3904046
## LogLoss:  0.4681621
## Mean Per-Class Error:  0.2235214
## AUC:  0.8433272
## pr_auc:  0.7168051
## Gini:  0.6866545
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##          0   1    Error      Rate
## 0      292 101 0.256997  =101/393
## 1       42 179 0.190045   =42/221
## Totals 334 280 0.232899  =143/614
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.321254 0.714571 215
## 2                       max f2  0.207900 0.801127 265
## 3                 max f0point5  0.543051 0.716544 137
## 4                 max accuracy  0.447269 0.789902 174
## 5                max precision  0.894283 0.866667  11
## 6                   max recall  0.047311 1.000000 378
## 7              max specificity  0.930334 0.997455   0
## 8             max absolute_mcc  0.447269 0.548323 174
## 9   max min_per_class_accuracy  0.381515 0.769231 196
## 10 max mean_per_class_accuracy  0.321254 0.776479 215
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## Cross-Validation Metrics Summary: 
##                               mean          sd cv_1_valid cv_2_valid
## accuracy                 0.7899374 0.015376968  0.7642276  0.8130081
## auc                      0.8452549 0.010865098  0.8283333  0.8708285
## err                     0.21006264 0.015376968 0.23577236 0.18699187
## err_count                     25.8   1.9183326       29.0       23.0
## f0point5                 0.6964409 0.013565934  0.6902985 0.72265625
## f1                      0.73873866 0.010146432  0.7184466  0.7628866
## f2                       0.7873838 0.017920174 0.74898785 0.80786026
## lift_top_group           2.2344697  0.50404537     2.5625  2.7954545
## logloss                 0.46815622 0.015717598 0.49344352 0.43153968
## max_per_class_error     0.22997828 0.029588874       0.24 0.20253165
## mcc                      0.5758232 0.024056237  0.5208032  0.6179175
## mean_per_class_accuracy 0.79732084 0.013610335  0.7654167  0.8191887
## mean_per_class_error    0.20267917 0.013610335 0.23458333 0.18081127
## mse                     0.15241112 0.005704805 0.16299967 0.13980189
## precision                0.6711169 0.017588206  0.6727273  0.6981132
## r2                      0.33725584    0.021736  0.3149939 0.39152396
## recall                   0.8242172  0.02852856  0.7708333 0.84090906
## rmse                    0.39026093  0.00733249 0.40373218  0.3739009
## specificity              0.7704245 0.029766915       0.76 0.79746836
##                         cv_3_valid cv_4_valid cv_5_valid
## accuracy                 0.7642276   0.796748  0.8114754
## auc                     0.85242164  0.8317031  0.8429878
## err                     0.23577236 0.20325203 0.18852459
## err_count                     29.0       25.0       23.0
## f0point5                0.66445184 0.70564514  0.6991525
## f1                      0.73394495  0.7368421 0.74157304
## f2                       0.8196721  0.7709251  0.7894737
## lift_top_group           1.3666667  1.3977273       3.05
## logloss                 0.46212325  0.4891477 0.46452695
## max_per_class_error     0.30769232 0.20454545 0.19512194
## mcc                      0.5603516  0.5768817 0.60316193
## mean_per_class_accuracy  0.7905983 0.79646146   0.814939
## mean_per_class_error    0.20940171 0.20353855 0.18506098
## mse                     0.15069951 0.15899125  0.1495633
## precision                    0.625  0.6862745 0.67346936
## r2                       0.3504465 0.30800384  0.3213109
## recall                   0.8888889 0.79545456      0.825
## rmse                    0.38820034 0.39873707 0.38673416
## specificity              0.6923077 0.79746836 0.80487806</code></pre>
<pre class="r"><code>pred &lt;- h2o.predict(aml, test) # predicting on test</code></pre>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre class="r"><code>h2o.performance(model = aml@leader, newdata = test) # performance of a model</code></pre>
<pre><code>## H2OBinomialMetrics: xgboost
## 
## MSE:  0.1706974
## RMSE:  0.4131554
## LogLoss:  0.5033908
## Mean Per-Class Error:  0.261384
## AUC:  0.7939948
## pr_auc:  0.5639243
## Gini:  0.5879897
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##         0  1    Error     Rate
## 0      67 40 0.373832  =40/107
## 1       7 40 0.148936    =7/47
## Totals 74 80 0.305195  =47/154
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.252897 0.629921  79
## 2                       max f2  0.122288 0.785953 110
## 3                 max f0point5  0.552369 0.603015  37
## 4                 max accuracy  0.552369 0.759740  37
## 5                max precision  0.913997 1.000000   0
## 6                   max recall  0.122288 1.000000 110
## 7              max specificity  0.913997 1.000000   0
## 8             max absolute_mcc  0.252897 0.439855  79
## 9   max min_per_class_accuracy  0.383294 0.702128  61
## 10 max mean_per_class_accuracy  0.252897 0.738616  79
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`</code></pre>
<pre class="r"><code>h2o.shutdown(prompt = F)</code></pre>
<pre><code>## [1] TRUE</code></pre>
